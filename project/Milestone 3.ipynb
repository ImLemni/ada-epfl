{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We complete the analysis made in the previous milestone with more in depth and complete content. We did not change too much the data handling part, except the matching part with wikipedia ressources. We added a few more rules to the cleaning process of initial strings that allow us to match much more titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : résumé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports and constants\n",
    "import pandas as pd\n",
    "from ada import data\n",
    "import numpy as np\n",
    "from ada.progressbar import ProgressBar\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import seaborn as sb\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We load previously saved files containing the filtered and clean data\n",
    "books_df = pd.read_json(data.get_path(\n",
    "    \"merged_clean_Books\", use_gzip=False), orient=\"records\")\n",
    "movies_df = pd.read_json(data.get_path(\n",
    "    \"merged_clean_Movies\", use_gzip=False), orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def split_sentiment(x):\n",
    "    \"\"\"\n",
    "    Split a dictionary of sentiment polarity into a serie.\n",
    "    - `neg`: negative\n",
    "    - `neu`: neutral\n",
    "    - `pos`: positive\n",
    "    - `compound`: compound\n",
    "    \"\"\"\n",
    "    return pd.Series([x['neg'], x['neu'], x['pos'], x['compound']])\n",
    "\n",
    "def plotCor(df1, df2, y, x, title, suffixa=\" for movies\", suffixb=\" for books\"):\n",
    "    \"\"\"\n",
    "    Plot 2 regplots side by sidethat share x and y axis with 10 bins on each\n",
    "    \"\"\"\n",
    "    zone, (plot1, plot2) = plt.subplots(ncols=2, sharey=True, sharex=True)\n",
    "    zone.set_size_inches(16, 6)\n",
    "\n",
    "    plot1 = sb.regplot(y=y, x=x, data=df1, ax=plot1, x_bins=10)\n",
    "    plot1.set_title(title + suffixa)\n",
    "\n",
    "    plot2 = sb.regplot(y=y, x=x, data=df2, ax=plot2, x_bins=10)\n",
    "    plot2.set_title(title + suffixb)\n",
    "    plt.show()\n",
    "\n",
    "def normalizeColumn(df, columns):\n",
    "    \"\"\"\n",
    "    Normalize a list of columns between 0 and 1 and return the modified\n",
    "    dataframe. \n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        df[column] = (df[column] - df[column].min()) / \\\n",
    "            (df[column].max() - df[column].min())\n",
    "    return df\n",
    "\n",
    "def plot_impact_time(df,groupby, main_col):\n",
    "    \"\"\"\n",
    "    Compute a new dataframe containing the number of occurence of reviews made for a book before/after/the same day \n",
    "    than the review made for a movie for the same franchise by the same user. \n",
    "    Show a barplot of this new dataframe\n",
    "    \"\"\"\n",
    "    time_impact_df_bf = df.groupby(groupby)[[\"unixReviewTime_movies\", \"unixReviewTime_books\"]].apply(\n",
    "        lambda x: x[x[\"unixReviewTime_movies\"] > x[\"unixReviewTime_books\"]].count() / x.shape[0])\n",
    "    time_impact_df_bf = time_impact_df_bf.reset_index().drop(\n",
    "        time_impact_df_bf.columns[len(time_impact_df_bf.columns) - 1], axis=1)\n",
    "    time_impact_df_bf.columns = [main_col, \"book before movie\"]\n",
    "\n",
    "    time_impact_df_at = df.groupby(groupby)[[\"unixReviewTime_movies\", \"unixReviewTime_books\"]].apply(\n",
    "        lambda x: x[x[\"unixReviewTime_movies\"] < x[\"unixReviewTime_books\"]].count() / x.shape[0])\n",
    "    time_impact_df_at = time_impact_df_at.reset_index().drop(\n",
    "        time_impact_df_at.columns[len(time_impact_df_at.columns) - 1], axis=1)\n",
    "    time_impact_df_at.columns = [main_col, \"book after movie\"]\n",
    "\n",
    "    time_impact_df_se = df.groupby(groupby)[[\"unixReviewTime_movies\", \"unixReviewTime_books\"]].apply(\n",
    "        lambda x: x[x[\"unixReviewTime_movies\"] == x[\"unixReviewTime_books\"]].count() / x.shape[0])\n",
    "    time_impact_df_se = time_impact_df_se.reset_index().drop(\n",
    "        time_impact_df_se.columns[len(time_impact_df_se.columns) - 1], axis=1)\n",
    "    time_impact_df_se.columns = [main_col, \"book same day movie\"]\n",
    "\n",
    "    time_impact_df = time_impact_df_bf.merge(\n",
    "        time_impact_df_at).merge(time_impact_df_se)\n",
    "    time_impact_df.plot(kind=\"bar\", x=main_col)\n",
    "    plt.show()\n",
    "    \n",
    "def same_plot_bar(dflist, namelist, xlegend):\n",
    "    \"\"\"\n",
    "    Show a single bar plot for a list of dataframe which share the same columns\n",
    "    This plot allow a good comparison between them. Colors are the default ones of matplotlib\n",
    "    \"\"\"\n",
    "    pos = list(range(len(dflist[0].mean())))\n",
    "    width = 1.0 / (1 + len(dflist))\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    for numb, df in enumerate(dflist):\n",
    "        plt.bar([p + numb * width for p in pos],\n",
    "                df.mean(),\n",
    "                width,\n",
    "                color='C' + str(numb),\n",
    "                yerr=df.std())\n",
    "\n",
    "    ax.set_xticks([p + len(dflist) * 0.25 * width for p in pos])\n",
    "    ax.set_xticklabels(xlegend)\n",
    "    plt.legend(namelist, loc='upper left')\n",
    "    plt.axhline(0, color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute a sentiment analysis for all reviews using the `vaderSentiment` package, as this package give a good analysis without too much overhead. We will mainly use the `compound` value, which is according to the documentation :\n",
    ">the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. \n",
    ">Calling it a 'normalized, weighted composite score' is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>description</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>price</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001472933</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>26.55</td>\n",
       "      <td>I attended an in depth study on Daniel last Fa...</td>\n",
       "      <td>AFA85QZQS1NKX</td>\n",
       "      <td>Great read.</td>\n",
       "      <td>The Book of Daniel</td>\n",
       "      <td>1375401600</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.618, 'pos': 0.382, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001472933</td>\n",
       "      <td>None</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>26.55</td>\n",
       "      <td>This book contains information which would hel...</td>\n",
       "      <td>ARFK2WYVYT1QQ</td>\n",
       "      <td>The book of Daniel</td>\n",
       "      <td>The Book of Daniel</td>\n",
       "      <td>1347321600</td>\n",
       "      <td>{'neg': 0.056, 'neu': 0.691, 'pos': 0.254, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001472933</td>\n",
       "      <td>None</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>5</td>\n",
       "      <td>26.55</td>\n",
       "      <td>This is a classic Larkin book great for Bible ...</td>\n",
       "      <td>A1O5WPLJSI7Y1B</td>\n",
       "      <td>The Book of Daniel</td>\n",
       "      <td>The Book of Daniel</td>\n",
       "      <td>1233532800</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001472933</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>26.55</td>\n",
       "      <td>Book in like-new condition, I paid a total aro...</td>\n",
       "      <td>A1ABJCJ06H9363</td>\n",
       "      <td>but worth of your time to finish these 250 pag...</td>\n",
       "      <td>The Book of Daniel</td>\n",
       "      <td>1404345600</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001472933</td>\n",
       "      <td>None</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>5</td>\n",
       "      <td>26.55</td>\n",
       "      <td>The Best explanation and Exposition of the Boo...</td>\n",
       "      <td>AO93WR5UAFA6F</td>\n",
       "      <td>The Book of Daniel</td>\n",
       "      <td>The Book of Daniel</td>\n",
       "      <td>1247788800</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin description   helpful  overall  price  \\\n",
       "0  0001472933        None       NaN        5  26.55   \n",
       "1  0001472933        None  1.000000        5  26.55   \n",
       "2  0001472933        None  0.777778        5  26.55   \n",
       "3  0001472933        None       NaN        5  26.55   \n",
       "4  0001472933        None  0.750000        5  26.55   \n",
       "\n",
       "                                          reviewText      reviewerID  \\\n",
       "0  I attended an in depth study on Daniel last Fa...   AFA85QZQS1NKX   \n",
       "1  This book contains information which would hel...   ARFK2WYVYT1QQ   \n",
       "2  This is a classic Larkin book great for Bible ...  A1O5WPLJSI7Y1B   \n",
       "3  Book in like-new condition, I paid a total aro...  A1ABJCJ06H9363   \n",
       "4  The Best explanation and Exposition of the Boo...   AO93WR5UAFA6F   \n",
       "\n",
       "                                             summary               title  \\\n",
       "0                                        Great read.  The Book of Daniel   \n",
       "1                                 The book of Daniel  The Book of Daniel   \n",
       "2                                 The Book of Daniel  The Book of Daniel   \n",
       "3  but worth of your time to finish these 250 pag...  The Book of Daniel   \n",
       "4                                 The Book of Daniel  The Book of Daniel   \n",
       "\n",
       "   unixReviewTime                                          sentiment  \n",
       "0      1375401600  {'neg': 0.0, 'neu': 0.618, 'pos': 0.382, 'comp...  \n",
       "1      1347321600  {'neg': 0.056, 'neu': 0.691, 'pos': 0.254, 'co...  \n",
       "2      1233532800  {'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compou...  \n",
       "3      1404345600  {'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'comp...  \n",
       "4      1247788800  {'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'comp...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "movies_df['sentiment'] = movies_df['reviewText'].apply(\n",
    "    lambda x: analyser.polarity_scores(x))\n",
    "books_df['sentiment'] = books_df['reviewText'].apply(\n",
    "    lambda x: analyser.polarity_scores(x))\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3509\n",
      "1535\n"
     ]
    }
   ],
   "source": [
    "movies_df[['neg', 'neu', 'pos', 'compound']\n",
    "          ] = movies_df['sentiment'].apply(split_sentiment)\n",
    "books_df[['neg', 'neu', 'pos', 'compound']\n",
    "         ] = books_df['sentiment'].apply(split_sentiment)\n",
    "\n",
    "print(len(books_df['asin'].unique()))\n",
    "print(len(movies_df['asin'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first filter the previous data to keep only reviews made by people who reviewed both a movie and a book of the same franchise. This will allow us to have a concrete comparison, without the bias inherent to the book/movie rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8793 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'franchise_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'franchise_id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-250bb4ee05dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers_both\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         movies_reviews_fr = movies_df[movies_df[\"reviewerID\"]\n\u001b[0;32m---> 10\u001b[0;31m                                       == user][\"franchise_id\"].unique()\n\u001b[0m\u001b[1;32m     11\u001b[0m         books_reviews_fr = books_df[books_df[\"reviewerID\"]\n\u001b[1;32m     12\u001b[0m                                     == user][\"franchise_id\"].unique()\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'franchise_id'"
     ]
    }
   ],
   "source": [
    "# Filter the users who gave reviews for a pair\n",
    "movies_users = movies_df['reviewerID'].unique()\n",
    "books_users = books_df['reviewerID'].unique()\n",
    "users_both = np.intersect1d(movies_users, books_users)\n",
    "len(users_both)\n",
    "\n",
    "user_same_franchise = []\n",
    "with ProgressBar(len(users_both)) as progress_bar:\n",
    "    for user in users_both:\n",
    "        movies_reviews_fr = movies_df[movies_df[\"reviewerID\"]\n",
    "                                      == user][\"franchise_id\"].unique()\n",
    "        books_reviews_fr = books_df[books_df[\"reviewerID\"]\n",
    "                                    == user][\"franchise_id\"].unique()\n",
    "        if len(np.intersect1d(movies_reviews_fr, books_reviews_fr)) > 0:\n",
    "            user_same_franchise.append({\"user\": user, \"franchises\": np.intersect1d(\n",
    "                movies_reviews_fr, books_reviews_fr)})\n",
    "        progress_bar.update(1)\n",
    "\n",
    "len(user_same_franchise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter the dataframes with these users\n",
    "# Take only reviews made by these users for the pair, not all their reviews\n",
    "mov_m = pd.DataFrame()\n",
    "book_m = pd.DataFrame()\n",
    "with ProgressBar(len(user_same_franchise)) as progress_bar:\n",
    "    for user_dict in user_same_franchise:\n",
    "        for fr in user_dict['franchises']:\n",
    "            mov_m = mov_m.append(movies_df[(movies_df[\"reviewerID\"] == user_dict['user']) & (\n",
    "                movies_df[\"franchise_id\"] == fr)])\n",
    "\n",
    "            book_m = book_m.append(books_df[(books_df[\"reviewerID\"] == user_dict['user']) & (\n",
    "                books_df[\"franchise_id\"] == fr)])\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now merge both dataframes (movies and books) so in a single row we have both reviews for a same pair for a single user, we will also normalize key features sucha as score or compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the datafraes with reviewerID and franchiseID as keys\n",
    "merged_df = mov_m.merge(book_m, on=[\"reviewerID\", \"franchise_id\"],\n",
    "                        suffixes=[\"_movies\", \"_books\"])\n",
    "merged_df = merged_df[['asin_movies', 'asin_books', 'reviewerID', 'franchise_id', \"overall_movies\",  \"neg_movies\", \"neu_movies\",\n",
    "                       \"pos_movies\", \"compound_movies\", \"overall_books\", \"neg_books\", \"neu_books\", \"pos_books\", \"compound_books\", \"unixReviewTime_books\", \"unixReviewTime_movies\"]]\n",
    "merged_df = normalizeColumn(\n",
    "    merged_df, [\"overall_movies\", \"overall_books\", \"compound_movies\", \"compound_books\"])\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to define categories of users behaviors related to the reviewing process of a pair movie/book. We will use a kmean clustering with 5 clusters, as it seems to be the more relevant in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df_fil = merged_df[[\"overall_movies\",\n",
    "                           \"overall_books\", \"compound_movies\", \"compound_books\"]]\n",
    "\n",
    "npdf = merged_df_fil.values\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=100).fit(npdf)\n",
    "print(kmeans.cluster_centers_)\n",
    "# Cluster 0 : good grades and reviews for movies and books\n",
    "# Cluster 1 : Bad review and bad grade for movies, good review and grade for books\n",
    "# Cluster 2 : Negative review foor book but positive review for movies\n",
    "# Cluster 3 : Bad grade for movies\n",
    "# Cluster 4 : Bad reviews for both movies and books\n",
    "kmeans_df = pd.DataFrame(np.c_[npdf, kmeans.labels_, merged_df[\"reviewerID\"].values], columns=[\n",
    "    \"overall_movies\", \"overall_books\", \"compound_movies\", \"compound_books\", \"cluster\", \"reviewerID\"])\n",
    "color_dict = {0.0: \"#fc8d59\", 1.0: \"#ffffbf\",\n",
    "              2.0: \"#91cf60\", 3.0: \"#ff2727\", 4.0: \"#000000\"}\n",
    "kmeans_df[\"color\"] = kmeans_df[\"cluster\"].map(color_dict)\n",
    "kmeans_df.head()\n",
    "\n",
    "# sb.regplot(data=kmeans_df, x=\"overall_movies\", y=\"overall_books\",\n",
    "#             scatter_kws={'c': kmeans_df['color']})\n",
    "# plt.show()\n",
    "scatterplot = kmeans_df.plot.scatter(\n",
    "    x=\"overall_movies\", y=\"overall_books\", figsize=(10, 10), c=kmeans_df['color'], s=200)\n",
    "plt.show()\n",
    "scatterplot = kmeans_df.plot.scatter(\n",
    "    x=\"compound_movies\", y=\"compound_books\", figsize=(10, 10), c=kmeans_df['color'], s=200)\n",
    "plt.show()\n",
    "scatterplot = kmeans_df.plot.scatter(\n",
    "    x=\"overall_books\", y=\"compound_books\", figsize=(10, 10), c=kmeans_df['color'], s=200)\n",
    "plt.show()\n",
    "scatterplot = kmeans_df.plot.scatter(\n",
    "    x=\"overall_movies\", y=\"compound_movies\", figsize=(10, 10), c=kmeans_df['color'], s=200)\n",
    "plt.show()\n",
    "\n",
    "threedee = plt.figure().gca(projection='3d')\n",
    "threedee.scatter(kmeans_df[\"overall_books\"],\n",
    "                 kmeans_df[\"compound_books\"], kmeans_df[\"overall_movies\"], c=kmeans_df['color'])\n",
    "threedee.set_xlabel('Overall score for books')\n",
    "threedee.set_ylabel('Sentiment for books')\n",
    "threedee.set_zlabel('Overall score for movies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TODO : remove some plots, re analyze clusters meaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does people who give many reviews always belong to the same cluster ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans_df[kmeans_df.duplicated(subset=[\n",
    "    \"overall_movies\", \"overall_books\", \"compound_movies\", \"compound_books\"], keep=False)]\n",
    "# Note that we have some duplicates, when looking at the reviews and products it is\n",
    "# just that often a same user copy paste its review for 2 distinct products\n",
    "\n",
    "multi_rating = kmeans_df[kmeans_df.duplicated(\n",
    "    subset=[\"reviewerID\"], keep=False)]\n",
    "# 38% of reviews are made by people who gave more than one review\n",
    "multi_rating.shape[0] / kmeans_df.shape[0]\n",
    "multi_rating_gb = pd.DataFrame()\n",
    "multi_rating_gb[\"unique\"] = multi_rating.groupby([\"reviewerID\"])[\n",
    "    \"cluster\"].nunique()\n",
    "multi_rating_gb[\"total\"] = multi_rating.groupby([\"reviewerID\"])[\n",
    "    \"cluster\"].count()\n",
    "multi_rating_gb = multi_rating_gb.reset_index()\n",
    "multi_rating_gb.plot.scatter(\n",
    "    x=\"total\", y=\"unique\", figsize=(10, 10))\n",
    "plt.show()\n",
    "\n",
    "list_user_unique_cluster = multi_rating_gb[multi_rating_gb[\"unique\"] == 1][\"reviewerID\"].tolist(\n",
    ")\n",
    "multi_rating[multi_rating[\"reviewerID\"].isin(\n",
    "    list_user_unique_cluster)].groupby([\"cluster\"])[\"reviewerID\"].nunique().plot(kind=\"bar\")\n",
    "plt.show()\n",
    "# We can see a big majority for cluster 0:\n",
    "# People who give good grades and reviews for both movies and books in a review\n",
    "# tends to do the same for all franchises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-dcf3816dee59>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-dcf3816dee59>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    TODO, recompute percentage and maybe change analysis\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "TODO, recompute percentage and maybe change analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the worst/better grades and reviews mention the book/movie ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mov_m_with_b = mov_m[mov_m[\"reviewText\"].str.contains(\"book|read\")]\n",
    "mov_m_with_b.shape[0] / mov_m.shape[0]\n",
    "book_m_with_b = book_m[book_m[\"reviewText\"].str.contains(\"film|movie\")]\n",
    "book_m_with_b.shape[0] / book_m.shape[0]\n",
    "# We have tried more complex combination but it is not realy accurate, for example if we add the word \"see\" for the books we add\n",
    "# many reviews but by looking at them we can see that it is not really relevant\n",
    "mov_m_with_b.describe()\n",
    "book_m_with_b.describe()\n",
    "plotCor(mov_m_with_b, mov_m, y=\"neg\", x=\"overall\", title=\"Negativity function of score\",\n",
    "        suffixa=\" for reviews speaking of the book\", suffixb=\" for any review\")\n",
    "plotCor(mov_m_with_b, mov_m, y=\"pos\", x=\"overall\", title=\"Positivity function of score\",\n",
    "        suffixa=\" for reviews speaking of the book\", suffixb=\" for any review\")\n",
    "\n",
    "plotCor(book_m_with_b, book_m, y=\"neg\", x=\"overall\", title=\"Negativity function of score \",\n",
    "        suffixa=\" for reviews speaking of the movie\", suffixb=\" for any review\")\n",
    "plotCor(book_m_with_b, book_m, y=\"pos\", x=\"overall\", title=\"Positivity function of score\",\n",
    "        suffixa=\" for reviews speaking of the movie\", suffixb=\" for any review\")\n",
    "\n",
    "\n",
    "# Let\"s also see which percentage contains those reference for each grade\n",
    "mov_m[\"reference_book\"] = mov_m[\"reviewText\"].str.contains(\n",
    "    \"book|read\").astype(int)\n",
    "mov_m.head()\n",
    "book_m[\"reference_movie\"] = book_m[\"reviewText\"].str.contains(\n",
    "    \"film|movie\").astype(int)\n",
    "mov_m_gb_ref = mov_m.groupby(['overall']).agg({'reference_book': 'sum'})[\n",
    "    \"reference_book\"] / mov_m.groupby(['overall']).agg({'reference_book': 'count'})[\"reference_book\"]\n",
    "mov_m_gb_ref.plot(kind='bar')\n",
    "plt.show()\n",
    "book_m_gb_ref = book_m.groupby(['overall']).agg({'reference_movie': 'sum'})[\n",
    "    \"reference_movie\"] / book_m.groupby(['overall']).agg({'reference_movie': 'count'})[\"reference_movie\"]\n",
    "book_m_gb_ref.plot(kind='bar')\n",
    "plt.show()\n",
    "cutSpace = np.linspace(-1, 1, 21)\n",
    "mov_m_sent_ref = mov_m.groupby(pd.cut(mov_m[\"compound\"], cutSpace)).agg({'reference_book': 'sum'})[\n",
    "    \"reference_book\"] / mov_m.groupby(pd.cut(mov_m[\"compound\"], cutSpace)).agg({'reference_book': 'count'})[\"reference_book\"]\n",
    "book_m_sent_ref = book_m.groupby(pd.cut(book_m[\"compound\"], cutSpace)).agg({'reference_movie': 'sum'})[\n",
    "    \"reference_movie\"] / book_m.groupby(pd.cut(book_m[\"compound\"], cutSpace)).agg({'reference_movie': 'count'})[\"reference_movie\"]\n",
    "mov_m_sent_ref.plot(kind=\"bar\")\n",
    "plt.show()\n",
    "book_m_sent_ref.plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact of the time : does reading the book before seing the movie as an impact ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books_bf_movies = merged_df[merged_df[\"unixReviewTime_books\"]\n",
    "                            < merged_df[\"unixReviewTime_movies\"]]\n",
    "books_at_movies = merged_df[merged_df[\"unixReviewTime_books\"]\n",
    "                            > merged_df[\"unixReviewTime_movies\"]]\n",
    "books_se_movies = merged_df[merged_df[\"unixReviewTime_books\"]\n",
    "                            == merged_df[\"unixReviewTime_movies\"]]\n",
    "\n",
    "print(f\"Review for book before review for movie : {100 * books_bf_movies.shape[0]/merged_df.shape[0]} %\")\n",
    "print(f\"Review for movie before review for book : {100 * books_at_movies.shape[0]/merged_df.shape[0]} %\")\n",
    "print(f\"Reviews the same day : {100 * books_se_movies.shape[0]/merged_df.shape[0]} %\")\n",
    "result_time_df = pd.DataFrame(columns=[\"Context\", \"Mean books\", \"Mean movies\", \"Mean compound_books\",\n",
    "                                       \"Mean compound_movies\", \"std books\", \"std_movies\", \"std comp books\", \"std comp movies\"])\n",
    "result_time_df.loc[0] = [\"Book before movie\", books_bf_movies[\"overall_books\"].mean(), books_bf_movies[\"overall_movies\"].mean(), books_bf_movies[\"compound_books\"].mean(), books_bf_movies[\"compound_movies\"].mean(),\n",
    "                         books_bf_movies[\"overall_books\"].std(), books_bf_movies[\"overall_movies\"].std(), books_bf_movies[\"compound_books\"].std(), books_bf_movies[\"compound_movies\"].std()]\n",
    "result_time_df.loc[1] = [\"Book before movie\", books_at_movies[\"overall_books\"].mean(), books_at_movies[\"overall_movies\"].mean(), books_at_movies[\"compound_books\"].mean(), books_at_movies[\"compound_movies\"].mean(),\n",
    "                         books_at_movies[\"overall_books\"].std(), books_at_movies[\"overall_movies\"].std(), books_at_movies[\"compound_books\"].std(), books_at_movies[\"compound_movies\"].std()]\n",
    "result_time_df.loc[2] = [\"Book before movie\", books_se_movies[\"overall_books\"].mean(), books_se_movies[\"overall_movies\"].mean(), books_se_movies[\"compound_books\"].mean(), books_se_movies[\"compound_movies\"].mean(),\n",
    "                         books_se_movies[\"overall_books\"].std(), books_se_movies[\"overall_movies\"].std(), books_se_movies[\"compound_books\"].std(), books_se_movies[\"compound_movies\"].std()]\n",
    "result_time_df\n",
    "\n",
    "zone, (plot1, plot2, plot3) = plt.subplots(ncols=3)\n",
    "zone.set_size_inches(16, 8)\n",
    "\n",
    "plot1 = sb.boxplot(y='overall_books', data=books_bf_movies, ax=plot1)\n",
    "plot1.set_title(\"Overall grades for books(book before movie)\")\n",
    "\n",
    "plot2 = sb.boxplot(y='overall_books', data=books_at_movies, ax=plot2)\n",
    "plot2.set_title(\"Overall grades for books(book after movie)\")\n",
    "\n",
    "plot3 = sb.boxplot(y='overall_books', data=books_se_movies, ax=plot3)\n",
    "plot3.set_title(\"Overall grades for books(book same movie)\")\n",
    "plt.show()\n",
    "\n",
    "# Let's try to see if there is a difference per grade\n",
    "(books_bf_movies.groupby([books_bf_movies[\"overall_books\"]])[\n",
    " \"overall_books\"].count() / books_bf_movies.shape[0]).plot(kind='bar')\n",
    "plt.show()\n",
    "(books_at_movies.groupby([books_at_movies[\"overall_books\"]])[\n",
    " \"overall_books\"].count() / books_at_movies.shape[0]).plot(kind='bar')\n",
    "plt.show()\n",
    "(books_se_movies.groupby([books_se_movies[\"overall_books\"]])[\n",
    " \"overall_books\"].count() / books_se_movies.shape[0]).plot(kind='bar')\n",
    "plt.show()\n",
    "plot_impact_time(merged_df,\"overall_books\", \"overall_books\")\n",
    "plot_impact_time(merged_df,\"overall_movies\", \"overall_movies\")\n",
    "plot_impact_time(merged_df,\n",
    "    pd.cut(merged_df[\"compound_books\"], np.linspace(-1, 1, 6)), \"compound_books\")\n",
    "plot_impact_time(merged_df,\n",
    "    pd.cut(merged_df[\"compound_movies\"], np.linspace(-1, 1, 6)), \"compound_movies\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact of the time : raw analysis on non filtered reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_df.groupby(pd.cut(movies_df[\"unixReviewTime\"], 10))[\"overall\"].mean()\n",
    "movies_df.groupby(pd.cut(movies_df[\"unixReviewTime\"], 10))[\"overall\"].std()\n",
    "movies_df[\"dateTime\"] = pd.to_datetime(\n",
    "    movies_df[\"unixReviewTime\"], unit=\"s\", origin='unix')\n",
    "movies_df.head()\n",
    "errors = movies_df.resample('M', on=\"dateTime\").std()\n",
    "errors_overall = errors[\"overall\"]\n",
    "movies_df.resample('M', on=\"dateTime\").mean().plot(\n",
    "    y=\"overall\", yerr=errors_overall)\n",
    "plt.show()\n",
    "errors_compound = errors[\"compound\"]\n",
    "movies_df.resample('M', on=\"dateTime\").mean().plot(\n",
    "    y=\"compound\", yerr=errors_compound)\n",
    "plt.show()\n",
    "\n",
    "# Time for grades/reviews for books\n",
    "books_df[\"dateTime\"] = pd.to_datetime(\n",
    "    books_df[\"unixReviewTime\"], unit=\"s\", origin='unix')\n",
    "b_errors = books_df.resample('M', on=\"dateTime\").std()\n",
    "b_errors_overall = b_errors[\"overall\"]\n",
    "books_df.resample('M', on=\"dateTime\").mean().plot(\n",
    "    y=\"overall\", yerr=b_errors_overall)\n",
    "plt.show()\n",
    "b_errors_compound = b_errors[\"compound\"]\n",
    "books_df.resample('M', on=\"dateTime\").mean().plot(\n",
    "    y=\"compound\", yerr=errors_compound)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People buying all or a majority of products linked to the same franchise_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_per_fr = merged_df.groupby([\"franchise_id\"])[\n",
    "    [\"asin_books\", \"asin_movies\"]].nunique()\n",
    "unique_per_fr[unique_per_fr[\"asin_books\"] + unique_per_fr[\"asin_movies\"] > 2]\n",
    "unique_per_user_per_fr = merged_df.groupby([\"reviewerID\", \"franchise_id\"])[\n",
    "    [\"asin_books\", \"asin_movies\"]].nunique()\n",
    "unique_per_user_per_fr[unique_per_user_per_fr[\"asin_books\"] +\n",
    "                       unique_per_user_per_fr[\"asin_movies\"] > 2]\n",
    "result_list_unique_fr = []\n",
    "for fr_id in unique_per_fr.index:\n",
    "    th = unique_per_fr.loc[fr_id][\"asin_books\"] + \\\n",
    "        unique_per_fr.loc[fr_id][\"asin_movies\"]\n",
    "\n",
    "    temp = unique_per_user_per_fr.xs(fr_id, level='franchise_id')\n",
    "    result_list_unique_fr += temp[(temp[\"asin_books\"] +\n",
    "                                   temp[\"asin_movies\"]) > max(0.5 * th, 2)].index.tolist()\n",
    "\n",
    "\n",
    "#  44 occurences of buying more than 50% of the franchise products\n",
    "len(result_list_unique_fr)\n",
    "len(set(result_list_unique_fr))\n",
    "\n",
    "buy_everything_df = merged_df[merged_df[\"reviewerID\"].isin(\n",
    "    result_list_unique_fr)]\n",
    "buy_everything_df.describe()\n",
    "buy_everything_df[[\"overall_movies\", \"overall_books\", \"compound_movies\", \"compound_books\"]].mean().plot.bar(\n",
    "    yerr=buy_everything_df[[\"overall_movies\", \"overall_books\", \"compound_movies\", \"compound_books\"]].std())\n",
    "plt.show()\n",
    "merged_df[[\"overall_movies\", \"overall_books\", \"compound_movies\", \"compound_books\"]].mean().plot.bar(\n",
    "    yerr=merged_df[[\"overall_movies\", \"overall_books\", \"compound_movies\", \"compound_books\"]].std())\n",
    "plt.show()\n",
    "same_plot_bar([buy_everything_df[[\"overall_movies\", \"overall_books\", \"compound_movies\", \"compound_books\"]], merged_df[[\n",
    "              \"overall_movies\", \"overall_books\", \"compound_movies\", \"compound_books\"]]], [\"Buy most\", \"All\"], [\"overall_movies\", \"overall_books\", \"compound_movies\", \"compound_books\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact of the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotCor(movies_df, books_df, \"overall\", \"price\",\n",
    "        \"Relation overall = f(price)\")  # already done in milestone2\n",
    "plotCor(movies_df, books_df, \"compound\", \"price\",\n",
    "        \"Relation compound = f(price)\")\n",
    "same_plot_bar([movies_df.groupby(pd.cut(movies_df[\"price\"], np.linspace(0, 40, 11)))[\"overall\"], books_df.groupby(\n",
    "    pd.cut(books_df[\"price\"], np.linspace(0, 40, 11)))[\"overall\"]], [\"movie grade\", \"book grade\"], np.linspace(0, 40, 11))\n",
    "same_plot_bar([movies_df.groupby(pd.cut(movies_df[\"price\"], np.linspace(0, 40, 11)))[\"compound\"], books_df.groupby(\n",
    "    pd.cut(books_df[\"price\"], np.linspace(0, 40, 11)))[\"compound\"]], [\"movie compound\", \"book compound\"], np.linspace(0, 40, 11))\n",
    "# Impact of the price for people who already buy other product of the franchise\n",
    "plotCor(mov_m, book_m, \"overall\", \"price\",\n",
    "        \"Relation overall = f(price)\")  # already done in milestone2\n",
    "plotCor(mov_m, book_m, \"compound\", \"price\",\n",
    "        \"Relation compound = f(price)\")\n",
    "same_plot_bar([mov_m.groupby(pd.cut(mov_m[\"price\"], np.linspace(0, 40, 11)))[\"overall\"], book_m.groupby(\n",
    "    pd.cut(book_m[\"price\"], np.linspace(0, 40, 11)))[\"overall\"]], [\"movie grade\", \"book grade\"], np.linspace(0, 40, 11))\n",
    "same_plot_bar([mov_m.groupby(pd.cut(mov_m[\"price\"], np.linspace(0, 40, 11)))[\"compound\"], book_m.groupby(\n",
    "    pd.cut(book_m[\"price\"], np.linspace(0, 40, 11)))[\"compound\"]], [\"movie compound\", \"book compound\"], np.linspace(0, 40, 11))\n",
    "\n",
    "\n",
    "same_plot_bar([mov_m.groupby(pd.cut(mov_m[\"price\"], np.linspace(0, 40, 11)))[\"overall\"], movies_df.groupby(\n",
    "    pd.cut(movies_df[\"price\"], np.linspace(0, 40, 11)))[\"overall\"]], [\"movie grade filtered\", \"movie grade all\"], np.linspace(0, 40, 11))\n",
    "same_plot_bar([mov_m.groupby(pd.cut(mov_m[\"price\"], np.linspace(0, 40, 11)))[\"compound\"], movies_df.groupby(\n",
    "    pd.cut(movies_df[\"price\"], np.linspace(0, 40, 11)))[\"compound\"]], [\"movie compound filtered\", \"movie compound all\"], np.linspace(0, 40, 11))\n",
    "\n",
    "same_plot_bar([book_m.groupby(pd.cut(book_m[\"price\"], np.linspace(0, 40, 11)))[\"overall\"], books_df.groupby(\n",
    "    pd.cut(books_df[\"price\"], np.linspace(0, 40, 11)))[\"overall\"]], [\"book grade filtered\", \"book grade all\"], np.linspace(0, 40, 11))\n",
    "same_plot_bar([book_m.groupby(pd.cut(book_m[\"price\"], np.linspace(0, 40, 11)))[\"compound\"], books_df.groupby(\n",
    "    pd.cut(books_df[\"price\"], np.linspace(0, 40, 11)))[\"compound\"]], [\"book compound filtered\", \"book compound all\"], np.linspace(0, 40, 11))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact of the product itself when there are more than one product for the same franchise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupby_fr_asin = movies_df.groupby([\"franchise_id\", \"asin\"])[\n",
    "    \"overall\", \"compound\"]\n",
    "same_plot_bar([groupby_fr_asin.mean(), groupby_fr_asin.median(),\n",
    "               groupby_fr_asin.min(), groupby_fr_asin.max()], [\"Mean\", \"Median\", \"Min\", \"Max\"], [\"overall\", \"compound\"])\n",
    "groupby_fr_asin_b = books_df.groupby([\"franchise_id\", \"asin\"])[\n",
    "    \"overall\", \"compound\"]\n",
    "same_plot_bar([groupby_fr_asin_b.mean(), groupby_fr_asin_b.median(),\n",
    "               groupby_fr_asin_b.min(), groupby_fr_asin_b.max()], [\"Mean\", \"Median\", \"Min\", \"Max\"], [\"overall\", \"compound\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
